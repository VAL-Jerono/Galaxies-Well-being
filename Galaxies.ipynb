{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VAL-Jerono/Galaxies-Well-being/blob/main/Galaxies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "119ffa24",
      "metadata": {
        "id": "119ffa24"
      },
      "source": [
        "- Problem Description:\n",
        "\n",
        "Data Description: You have data on 181 galaxies over a span of up to 26 years. There are 80 variables describing the demographic and socio-economic aspects of these galaxies, alongside a composite index measuring their well-being.\n",
        "\n",
        "- Objective: Your task is two-fold:\n",
        "\n",
        "\n",
        "Identify which variables (out of the 80) most significantly explain the variance in the well-being index.\n",
        "\n",
        "\n",
        "Predict the future values of the well-being index for a validation dataset with the highest level of certainty.\n",
        "\n",
        "\n",
        "- Tasks to Accomplish:\n",
        "- Variable Selection and Analysis:\n",
        "\n",
        "\n",
        "Conduct exploratory data analysis (EDA) to understand the data distribution, correlations, and outliers.\n",
        "\n",
        "\n",
        "Use techniques like feature selection or dimensionality reduction to identify the variables that best explain the variance in the well-being index.\n",
        "\n",
        "- Future Well-Being Prediction:\n",
        "\n",
        "\n",
        "Train predictive models using the provided dataset to forecast the future well-being index for the validation dataset.\n",
        "\n",
        "\n",
        "Evaluate model performance using the Root Mean Squared Error (RMSE) metric to ensure accurate predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "584fbe00",
      "metadata": {
        "id": "584fbe00"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "159a08f5",
      "metadata": {
        "id": "159a08f5"
      },
      "source": [
        "MSc Data Science and Analytics 2025 Scholarship Exam\n",
        "## Galaxy Well-Being Analysis and Prediction\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Business Understanding\n",
        "\n",
        "**Objective:**  \n",
        "To determine the demographic and socio-economic variables that best explain the variance in the **Well-Being Index** of galaxies and predict future values of the index using provided data.\n",
        "\n",
        "**Deliverables:**\n",
        "- A 5-slide PDF report highlighting findings and key insights\n",
        "- A CSV file of future well-being index predictions (`ID`, `Pred_well_being_index`)\n",
        "- A fully commented and reproducible Python/R notebook\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Data Understanding\n",
        "\n",
        "**Initial Observations:**\n",
        "- Dataset: `Train_data.csv`\n",
        "- Rows: 3,097\n",
        "- Columns: 81 (80 features + 1 target: `Well-Being Index`)\n",
        "- Key fields: `ID`, `galaxy`, `galactic year`\n",
        "- Target Variable: Continuous (`Well-Being Index`), hence a regression problem\n",
        "- Data contains numerous demographic and socio-economic indicators\n",
        "- Several columns have missing values\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Data Preparation\n",
        "\n",
        "Planned Steps:\n",
        "- Handle missing values (e.g., imputation or removal)\n",
        "- Convert data types where needed\n",
        "- Detect and remove low-variance or irrelevant columns\n",
        "- Normalize or scale features if required\n",
        "- Feature engineering (e.g., interactions, ratios)\n",
        "- Split into training/validation sets if applicable\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Modeling\n",
        "\n",
        "Candidate Models:\n",
        "- Linear Regression (for baseline)\n",
        "- Random Forest Regressor\n",
        "- Gradient Boosted Trees (e.g., XGBoost or LightGBM)\n",
        "- Ridge/Lasso Regression\n",
        "\n",
        "Feature Selection Techniques:\n",
        "- Correlation Analysis\n",
        "- Recursive Feature Elimination (RFE)\n",
        "- Tree-based model feature importances\n",
        "\n",
        "Evaluation Metric:\n",
        "- Root Mean Squared Error (RMSE), as per instructions\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Evaluation\n",
        "\n",
        "Steps:\n",
        "- Compare models on validation data using RMSE\n",
        "- Analyze prediction accuracy and generalization\n",
        "- Use feature importance plots to determine key drivers of well-being\n",
        "\n",
        "Deliver:\n",
        "- Clear narrative explaining which features contribute most to variance in the Well-Being Index\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Deployment (Submission Prep)\n",
        "\n",
        "Final Submission Folder: `firstname_lastname.zip`, to include:\n",
        "1. **PDF Report** — Max 5 slides, presenting analysis and insights\n",
        "2. **Predictions CSV** — Named `firstname_lastname_DSA.csv`\n",
        "3. **Python/R Notebook** — Clean, reproducible, well-commented code with:\n",
        "   - EDA\n",
        "   - Modeling\n",
        "   - Evaluation\n",
        "   - Visualizations\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f75587a",
      "metadata": {
        "id": "0f75587a"
      },
      "source": [
        "### 2. Data Understanding\n",
        "\n",
        "**Initial Observations:**\n",
        "- Dataset: `Train_data.csv`\n",
        "- Rows: 3,097\n",
        "- Columns: 81 (80 features + 1 target: `Well-Being Index`)\n",
        "- Key fields: `ID`, `galaxy`, `galactic year`\n",
        "- Target Variable: Continuous (`Well-Being Index`), hence a regression problem\n",
        "- Data contains numerous demographic and socio-economic indicators\n",
        "- Several columns have missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c83b6465",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "c83b6465",
        "outputId": "c39be71d-dfdb-4263-b0f9-54a6ae46018a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Data/Train_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5523cb15655c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/Train_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/Train_data.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('Data/Train_data.csv')\n",
        "info = df.info()\n",
        "desc = df.describe()\n",
        "head = df.head()\n",
        "info, desc,\n",
        "\n",
        "df.shape, df.columns[:10], head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e2de53c",
      "metadata": {
        "id": "7e2de53c"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1accf596",
      "metadata": {
        "id": "1accf596"
      },
      "outputs": [],
      "source": [
        "# Display summary statistics and check for missing values\n",
        "summary_stats = df.describe(include='all').T\n",
        "missing_values = df.isnull().sum().sort_values(ascending=False)\n",
        "missing_percent = (missing_values / len(df)) * 100\n",
        "\n",
        "# Combine missing values into a single DataFrame\n",
        "missing_summary = pd.DataFrame({\n",
        "    'Missing Values': missing_values,\n",
        "    'Percentage (%)': missing_percent\n",
        "})\n",
        "missing_summary = missing_summary[missing_summary['Missing Values'] > 0]\n",
        "\n",
        "# Identify data types and number of unique values per column\n",
        "data_types = df.dtypes\n",
        "unique_counts = df.nunique()\n",
        "\n",
        "column_summary = pd.DataFrame({\n",
        "    'Data Type': data_types,\n",
        "    'Unique Values': unique_counts,\n",
        "    'Missing Values': missing_values,\n",
        "    'Missing %': missing_percent\n",
        "}).sort_values(by='Missing %', ascending=False)\n",
        "\n",
        "column_summary.head(10)  # Show top 15 columns with highest missing %\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "45e5f805",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "45e5f805",
        "outputId": "21a59dbf-52b8-4511-8b3d-6695b98ae807"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-27f120f7c1ec>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#1.number of rows and columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#2. Check column data types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "#1.number of rows and columns\n",
        "num_rows, num_columns = df.shape\n",
        "\n",
        "#2. Check column data types\n",
        "data_types = df.dtypes.value_counts()\n",
        "\n",
        "#3. How many columns have missing data and how much)\n",
        "total_missing = df.isnull().sum().sum()\n",
        "columns_with_missing = df.isnull().sum()\n",
        "num_columns_with_missing = (columns_with_missing > 0).sum()\n",
        "\n",
        "# # 4. Summary statistics for numeric variables\n",
        "# basic_stats = df.describe().T[['mean', 'std', 'min', 'max']].round(2)\n",
        "\n",
        "# # 5. Identify columns with only one unique value (not useful for modeling)\n",
        "# constant_columns = [col for col in df.columns if df[col].nunique() == 1]\n",
        "\n",
        "{\n",
        "    \"Shape (Rows, Columns)\": (num_rows, num_columns),\n",
        "    \"Data Types Count\": data_types.to_dict(),\n",
        "    \"Total Missing Values\": total_missing,\n",
        "    \"Columns with Missing Values\": num_columns_with_missing,\n",
        "    #\"Columns with Only One Unique Value\": constant_columns[:5],  # showing first 5 if many\n",
        "    #\"Basic Statistics (sample)\": basic_stats.head(5)  # show stats for first 5 columns\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "432abb1b",
      "metadata": {
        "id": "432abb1b"
      },
      "outputs": [],
      "source": [
        "missing_values = df.isnull().sum().sort_values(ascending=False)\n",
        "missing_percent = (missing_values / len(df)) * 100\n",
        "\n",
        "\n",
        "# Identify data types and number of unique values per column\n",
        "data_types = df.dtypes\n",
        "unique_counts = df.nunique()\n",
        "\n",
        "\n",
        "column_summary = pd.DataFrame({\n",
        "    'Data Type': data_types,\n",
        "    'Unique Values': unique_counts,\n",
        "    'Missing Values': missing_values,\n",
        "    'Missing %': missing_percent\n",
        "}).sort_values(by='Missing %', ascending=False)\n",
        "\n",
        "column_summary.head(15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "328080f6",
      "metadata": {
        "id": "328080f6"
      },
      "source": [
        "### Data Understanding Summary\n",
        "\n",
        "Here’s what we’ve uncovered so far:\n",
        "\n",
        "---\n",
        "\n",
        "#### 🧾 General Dataset Structure\n",
        "\n",
        "* **Total observations (rows):** 3,097\n",
        "* **Total features (columns):** 81\n",
        "* **Target variable:** `Well-Being Index`\n",
        "* **Other key columns:** `ID`, `galaxy`, `galactic year`\n",
        "\n",
        "---\n",
        "\n",
        "#### Missing Data Overview\n",
        "\n",
        "Several variables have **extremely high missingness**, particularly:\n",
        "\n",
        "| Column                                               | Missing % |\n",
        "| ---------------------------------------------------- | --------- |\n",
        "| `Current health expenditure (% of GGP)`              | 90.05%    |\n",
        "| `Interstellar Data Net users (% of population)`      | 90.02%    |\n",
        "| `Interstellar phone subscriptions (per 100 people)`  | 89.76%    |\n",
        "| `Respiratory disease incidence (per 100,000 people)` | 89.64%    |\n",
        "| `Gender Inequality Index (GII)`                      | 88.47%    |\n",
        "| ...                                                  | ...       |\n",
        "\n",
        "* More than **10 features have >85% missing values**\n",
        "* These may need to be **dropped** or **imputed** with caution, depending on their relevance and correlation with the target\n",
        "\n",
        "---\n",
        "\n",
        "#### Data Types and Cardinality\n",
        "\n",
        "* Almost all columns are `float64` (numerical)\n",
        "* No obvious categorical variables except for possibly `galaxy` and `galactic year`\n",
        "* Most features are continuous and can be scaled or transformed if necessary\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "61e36dae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "61e36dae",
        "outputId": "ae329cfd-b394-49ab-c2f5-177503f96a70"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0ad7e1957b3a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 1. Visualize missingness (top 20 missing columns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmissing_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Visualize missingness (top 20 missing columns)\n",
        "missing_data = df.isnull().mean().sort_values(ascending=False).head(15)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=missing_data.values * 100, y=missing_data.index, palette=\"magma\")\n",
        "plt.xlabel('% Missing Values')\n",
        "plt.ylabel('Columns')\n",
        "plt.title('Top 20 Columns with Most Missing Values')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Correlation with target variable\n",
        "correlation_with_target = df.corr(numeric_only=True)['Well-Being Index'].drop('Well-Being Index')\n",
        "top_corr = correlation_with_target.abs().sort_values(ascending=False).head(15)\n",
        "\n",
        "# Barplot of most correlated features (absolute values)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_corr.values, y=top_corr.index, palette=\"viridis\")\n",
        "plt.title('Top 15 Features Most Correlated with Well-Being Index')\n",
        "plt.xlabel('Absolute Correlation')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Also return top positively and negatively correlated variables for context\n",
        "top_positive = correlation_with_target.sort_values(ascending=False).head(5)\n",
        "top_negative = correlation_with_target.sort_values().head(5)\n",
        "\n",
        "top_positive, top_negative\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47f1f748",
      "metadata": {
        "id": "47f1f748"
      },
      "outputs": [],
      "source": [
        "# Also return top positively and negatively correlated variables for context\n",
        "top_positive = correlation_with_target.sort_values(ascending=False).head(20)\n",
        "top_negative = correlation_with_target.sort_values().head(20)\n",
        "\n",
        "top_positive, top_negative\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46f688a2",
      "metadata": {
        "id": "46f688a2"
      },
      "source": [
        "### Visual Summary of Data Quality and Insights\n",
        "\n",
        "---\n",
        "\n",
        "#### 🔍 1. Missing Data Overview\n",
        "\n",
        "We visualized the top 20 columns with the most missing values. Key takeaways:\n",
        "\n",
        "* Several variables like `Current health expenditure` and `Interstellar phone subscriptions` have **>85% missing values**\n",
        "* These may need to be **dropped or imputed** cautiously\n",
        "\n",
        "---\n",
        "\n",
        "#### 📈 2. Correlation with `Well-Being Index`\n",
        "\n",
        "We checked how strongly each feature is correlated with the target. Here are the **top drivers**:\n",
        "\n",
        "---\n",
        "\n",
        "##### 🟢 Top 5 Positively Correlated Features\n",
        "\n",
        "| Feature                                         | Correlation |\n",
        "| ----------------------------------------------- | ----------- |\n",
        "| `Old age dependency ratio`                      | **+0.74**   |\n",
        "| `Interstellar Data Net users (% of population)` | **+0.72**   |\n",
        "| `IDI, female`                                   | +0.69       |\n",
        "| `IDI, male`                                     | +0.68       |\n",
        "| `IDI (overall)`                                 | +0.66       |\n",
        "\n",
        "> These suggest that **aging populations**, **digital access**, and **development indices** are strongly associated with higher well-being.\n",
        "\n",
        "---\n",
        "\n",
        "##### 🔴 Top 5 Negatively Correlated Features\n",
        "\n",
        "| Feature                         | Correlation |\n",
        "| ------------------------------- | ----------- |\n",
        "| `Gender Inequality Index (GII)` | **-0.80**   |\n",
        "| `IDI male, Rank`                | -0.72       |\n",
        "| `IDI Rank`                      | -0.71       |\n",
        "| `IDI female, Rank`              | -0.70       |\n",
        "| `Young age dependency ratio`    | -0.64       |\n",
        "\n",
        "> So, **higher gender inequality** and **youth burden** tend to lower well-being.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0138dda8",
      "metadata": {
        "id": "0138dda8"
      },
      "outputs": [],
      "source": [
        "# Get features with more than 85% missing values\n",
        "high_missing = df.isnull().mean()\n",
        "high_missing_cols = high_missing[high_missing > 0.70].index\n",
        "\n",
        "# Check their correlation with the Well-Being Index\n",
        "high_missing_corr = df[high_missing_cols].corrwith(df['Well-Being Index']).dropna()\n",
        "\n",
        "# Sort by absolute correlation\n",
        "high_missing_corr_sorted = high_missing_corr.reindex(high_missing_corr.abs().sort_values(ascending=False).index)\n",
        "high_missing_corr_sorted\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ea2d02",
      "metadata": {
        "id": "25ea2d02"
      },
      "outputs": [],
      "source": [
        "top_positive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9cdfbeb",
      "metadata": {
        "id": "c9cdfbeb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4628ca6e",
      "metadata": {
        "id": "4628ca6e"
      },
      "outputs": [],
      "source": [
        "high_missing_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65b97913",
      "metadata": {
        "id": "65b97913"
      },
      "outputs": [],
      "source": [
        "high_missing_corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c89f8a8",
      "metadata": {
        "id": "3c89f8a8"
      },
      "outputs": [],
      "source": [
        "# Get features with more than 85% missing values\n",
        "high_missing = df.isnull().mean()\n",
        "high_missing_cols = high_missing[high_missing > 0.80].index\n",
        "\n",
        "# Check their correlation with the Well-Being Index\n",
        "high_missing_corr = df[high_missing_cols].corrwith(df['Well-Being Index']).dropna()\n",
        "\n",
        "# Sort by absolute correlation\n",
        "high_missing_corr_sorted = high_missing_corr.reindex(high_missing_corr.abs().sort_values(ascending=False).index)\n",
        "high_missing_corr_sorted\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54a5de3a",
      "metadata": {
        "id": "54a5de3a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6f7018e",
      "metadata": {
        "id": "f6f7018e"
      },
      "outputs": [],
      "source": [
        "# Step 1: Strip whitespace from column names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Step 2: Drop columns with more than 90% missing values\n",
        "threshold = 0.6 * len(df)\n",
        "df_cleaned = df.dropna(thresh=threshold, axis=1)\n",
        "\n",
        "# Step 3: Strip whitespace from string values where applicable\n",
        "df_cleaned = df_cleaned.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "\n",
        "# Step 4: Convert percentage columns to numeric (removing % and converting to float)\n",
        "percentage_cols = [col for col in df_cleaned.columns if '%' in col]\n",
        "for col in percentage_cols:\n",
        "    df_cleaned[col] = df_cleaned[col].replace('[^\\d.]', '', regex=True).astype(float)\n",
        "\n",
        "# Step 5: Provide summary of cleaned data\n",
        "summary = {\n",
        "    \"Original column count\": df.shape[1],\n",
        "    \"Remaining column count\": df_cleaned.shape[1],\n",
        "    \"Removed columns\": list(set(df.columns) - set(df_cleaned.columns)),\n",
        "    \"Remaining missing values (per column)\": df_cleaned.isnull().sum().sort_values(ascending=False).head(10)\n",
        "}\n",
        "\n",
        "df_cleaned.head(), summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab66c78e",
      "metadata": {
        "id": "ab66c78e"
      },
      "outputs": [],
      "source": [
        "# Recompute missing value percentages for all columns\n",
        "missing_percentages = df.isnull().mean() * 100\n",
        "\n",
        "# Filter columns that were dropped due to >90% missing data\n",
        "dropped_columns = list(set(df.columns) - set(df_cleaned.columns))\n",
        "dropped_columns_missing = missing_percentages[dropped_columns].sort_values(ascending=False)\n",
        "\n",
        "dropped_columns_missing.head(60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "462d0c28",
      "metadata": {
        "id": "462d0c28"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Step 1: Impute missing numerical values with mean\n",
        "num_cols_with_na = df_cleaned.select_dtypes(include='number').columns[df_cleaned.isna().any()]\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df_cleaned[num_cols_with_na] = imputer.fit_transform(df_cleaned[num_cols_with_na])\n",
        "\n",
        "# Step 2: Save the cleaned and imputed data to a new CSV file\n",
        "cleaned_file_path = \"Data/Cleaned_Train_Data.csv\"\n",
        "df_cleaned.to_csv(cleaned_file_path, index=False)\n",
        "\n",
        "# Step 3: Generate explanation for column dropping\n",
        "dropped_columns_reason = {\n",
        "    \"reason\": \"Dropped columns contained more than 90% missing values, making them unreliable for modeling or analysis without introducing significant noise or bias. \"\n",
        "              \"These columns had insufficient data density to support robust imputation or insights.\"\n",
        "}\n",
        "\n",
        "# Step 4: Begin Exploratory Data Analysis (EDA)\n",
        "eda_summary = {\n",
        "    \"data_shape\": df_cleaned.shape,\n",
        "    \"data_types\": df_cleaned.dtypes,\n",
        "    \"descriptive_stats\": df_cleaned.describe().T,\n",
        "    \"correlation_matrix\": df_cleaned.corr(numeric_only=True)\n",
        "}\n",
        "\n",
        "cleaned_file_path, dropped_columns_reason, eda_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb816901",
      "metadata": {
        "id": "fb816901"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Correct identification of numeric columns with missing values\n",
        "numeric_cols = df_cleaned.select_dtypes(include='number').columns\n",
        "num_cols_with_na = [col for col in numeric_cols if df_cleaned[col].isna().any()]\n",
        "\n",
        "# Impute missing numeric values with the mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df_cleaned[num_cols_with_na] = imputer.fit_transform(df_cleaned[num_cols_with_na])\n",
        "\n",
        "# Save the cleaned and imputed data to a CSV file\n",
        "cleaned_file_path = \"Data/Cleaned_Train_Data.csv\"\n",
        "df_cleaned.to_csv(cleaned_file_path, index=False)\n",
        "\n",
        "# Generate reason for column dropping\n",
        "dropped_columns_reason = (\n",
        "    \"Columns were dropped because more than 90% of their values were missing. \"\n",
        "    \"Such sparse data offers little statistical power or meaningful contribution to modeling, \"\n",
        "    \"and imputing them would introduce more bias than value.\"\n",
        ")\n",
        "\n",
        "# Conduct Exploratory Data Analysis (EDA)\n",
        "eda_summary = {\n",
        "    \"Data Shape\": df_cleaned.shape,\n",
        "    \"Data Types\": df_cleaned.dtypes,\n",
        "    \"Descriptive Statistics\": df_cleaned.describe().T,\n",
        "    \"Correlation Matrix\": df_cleaned.corr(numeric_only=True)\n",
        "}\n",
        "\n",
        "cleaned_file_path, dropped_columns_reason, eda_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5975737",
      "metadata": {
        "id": "a5975737"
      },
      "outputs": [],
      "source": [
        "# Step 1: Drop columns with more than 70% missing values\n",
        "threshold = 0.3 * len(df)  # Keep columns with at least 30% non-null\n",
        "df_cleaned = df.loc[:, df.notnull().sum() >= threshold]\n",
        "\n",
        "# Step 2: Check remaining data types\n",
        "data_types = df_cleaned.dtypes\n",
        "\n",
        "# Separate numeric and categorical columns\n",
        "numeric_cols = df_cleaned.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_cols = df_cleaned.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Step 3: Impute missing values\n",
        "# Numeric: fill with median\n",
        "df_cleaned[numeric_cols] = df_cleaned[numeric_cols].apply(lambda x: x.fillna(x.median()))\n",
        "# Categorical: fill with mode\n",
        "for col in categorical_cols:\n",
        "    df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mode()[0])\n",
        "\n",
        "# Save cleaned dataset\n",
        "cleaned_file_path = \"Data/Train_data_cleaned.csv\"\n",
        "df_cleaned.to_csv(cleaned_file_path, index=False)\n",
        "\n",
        "cleaned_file_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45751507",
      "metadata": {
        "id": "45751507"
      },
      "outputs": [],
      "source": [
        "# Identify columns that were dropped\n",
        "dropped_columns = set(df.columns) - set(df_cleaned.columns)\n",
        "dropped_columns = list(dropped_columns)\n",
        "\n",
        "dropped_columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a01c76c",
      "metadata": {
        "id": "4a01c76c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df1 = pd.read_csv('Data/Train_data_cleaned.csv')\n",
        "info1 = df1.info()\n",
        "desc1 = df1.describe()\n",
        "head1 = df1.head()\n",
        "info1, desc1,\n",
        "\n",
        "df1.shape, df1.columns[:10], head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8c0c2d8",
      "metadata": {
        "id": "b8c0c2d8"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a47d5357",
      "metadata": {
        "id": "a47d5357"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style for plots\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Correlation matrix with Well-Being Index\n",
        "corr_matrix = df1.corr(numeric_only=True)\n",
        "wellbeing_corr = corr_matrix[\"Well-Being Index\"].sort_values(ascending=False)\n",
        "\n",
        "# Plot the correlation with Well-Being Index\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=wellbeing_corr.values, y=wellbeing_corr.index, palette='coolwarm')\n",
        "plt.title(\"Correlation of Features with Well-Being Index\")\n",
        "plt.xlabel(\"Correlation Coefficient\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "009ddc28",
      "metadata": {
        "id": "009ddc28"
      },
      "outputs": [],
      "source": [
        "# Determine the number of numeric columns (excluding ID)\n",
        "num_plots = len(numeric_cols)\n",
        "\n",
        "# Calculate required rows and columns\n",
        "cols = 3\n",
        "rows = (num_plots + cols - 1) // cols  # Ceiling division\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(16, 5 * rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot histograms\n",
        "for i, col in enumerate(numeric_cols):\n",
        "    sns.histplot(df[col], kde=True, ax=axes[i], color='skyblue')\n",
        "    axes[i].set_title(f'Distribution of {col}')\n",
        "    axes[i].set_xlabel('')\n",
        "    axes[i].set_ylabel('')\n",
        "\n",
        "# Remove unused subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "520f3766",
      "metadata": {
        "id": "520f3766"
      },
      "outputs": [],
      "source": [
        "# 2. Correlation with target variable\n",
        "correlation_with_target = df1.corr(numeric_only=True)['Well-Being Index'].drop('Well-Being Index')\n",
        "top_corr = correlation_with_target.abs().sort_values(ascending=False).head(15)\n",
        "\n",
        "# Barplot of most correlated features (absolute values)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_corr.values, y=top_corr.index, palette=\"viridis\")\n",
        "plt.title('Top 15 Features Most Correlated with Well-Being Index')\n",
        "plt.xlabel('Absolute Correlation')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Also return top positively and negatively correlated variables for context\n",
        "top_positive = correlation_with_target.sort_values(ascending=False).head(10)\n",
        "top_negative = correlation_with_target.sort_values().head(10)\n",
        "\n",
        "top_positive, top_negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "15fe49d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "15fe49d3",
        "outputId": "7a0a66ee-1401-430f-f597-6478d8ee3de0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Data/Train_data_cleaned.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d8869c1654ef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/Train_data_cleaned.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Intergalactic Development Index (IDI), Rank'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'galactic year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Well-Being Index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Well-Being Index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/Train_data_cleaned.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Load data\n",
        "df2 = pd.read_csv('Data/Train_data_cleaned.csv').drop(columns=['ID', 'Intergalactic Development Index (IDI), Rank','galactic year'])\n",
        "X = df2.drop(columns=['Well-Being Index'])\n",
        "y = df2['Well-Being Index']\n",
        "\n",
        "# # Preprocessing: One-hot encode 'galaxy' and scale numeric features\n",
        "# preprocessor = ColumnTransformer(\n",
        "#     transformers=[\n",
        "#         ('cat', OneHotEncoder(), ['galaxy'])\n",
        "#     ],\n",
        "#     remainder=StandardScaler()\n",
        "# )\n",
        "\n",
        "# Updated baseline model pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['galaxy'])  # Handle new categories\n",
        "    ],\n",
        "    remainder=StandardScaler()\n",
        ")\n",
        "\n",
        "# Baseline model pipeline\n",
        "baseline_model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "# Cross-validation\n",
        "baseline_scores = cross_val_score(baseline_model, X, y, cv=5, scoring='r2')\n",
        "baseline_r2 = baseline_scores.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe0689c2",
      "metadata": {
        "id": "fe0689c2"
      },
      "outputs": [],
      "source": [
        "pip install --user xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83c47ec7",
      "metadata": {
        "id": "83c47ec7"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# # Preprocessing for XGBoost\n",
        "# preprocessor_xgb = ColumnTransformer(\n",
        "#     transformers=[\n",
        "#         ('cat', OrdinalEncoder(), ['galaxy'])\n",
        "#     ],\n",
        "#     remainder='passthrough'\n",
        "# )\n",
        "\n",
        "# Updated XGBoost pipeline\n",
        "preprocessor_xgb = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['galaxy'])  # Use OHE instead\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# XGBoost pipeline\n",
        "xgb_pipe = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor_xgb),\n",
        "    ('regressor', XGBRegressor(random_state=42, n_jobs=-1))\n",
        "])\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'regressor__n_estimators': [100, 200, 300],\n",
        "    'regressor__max_depth': [3, 6, 9],\n",
        "    'regressor__learning_rate': [0.01, 0.1, 0.2],\n",
        "    'regressor__subsample': [0.8, 0.9, 1.0],\n",
        "    'regressor__colsample_bytree': [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "# Randomized search\n",
        "random_search = RandomizedSearchCV(\n",
        "    xgb_pipe,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=50,\n",
        "    cv=5,\n",
        "    scoring='r2',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "random_search.fit(X, y)\n",
        "\n",
        "# Best model evaluation\n",
        "best_xgb = random_search.best_estimator_\n",
        "xgb_scores = cross_val_score(best_xgb, X, y, cv=5, scoring='r2')\n",
        "xgb_r2 = xgb_scores.mean()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}